"""Gradio web interface for traffic equipment anomaly detection."""

import json
import os
import gradio as gr
from PIL import Image, ImageDraw, ImageFont

# å…¨å±€æ¨¡å‹å®ä¾‹
_model = None
_processor = None
_current_model_name = None

# åŸºç¡€æ¨¡å‹åˆ—è¡¨
BASE_MODELS = {
    "Qwen3-VL-2B": "./model_cache/Qwen/Qwen3-VL-2B-Instruct",
    "Qwen3-VL-4B": "./model_cache/Qwen/Qwen3-VL-4B-Instruct",
    "Qwen3-VL-8B": "./model_cache/Qwen/Qwen3-VL-8B-Instruct",
    "Qwen2.5-VL-7B": "./model_cache/Qwen/Qwen2.5-VL-7B-Instruct",
}

# å¾®è°ƒæ¨¡å‹ç›®å½•
FINETUNED_MODELS_DIR = "./outputs"


def scan_finetuned_models() -> dict:
    """æ‰«æ outputs ç›®å½•ä¸‹çš„å¾®è°ƒæ¨¡å‹"""
    finetuned_models = {}

    if not os.path.exists(FINETUNED_MODELS_DIR):
        return finetuned_models

    for name in os.listdir(FINETUNED_MODELS_DIR):
        model_path = os.path.join(FINETUNED_MODELS_DIR, name)
        config_path = os.path.join(model_path, "finetune_config.json")
        adapter_config_path = os.path.join(model_path, "adapter_config.json")

        # æ£€æŸ¥æ˜¯å¦æœ‰ LoRA é…ç½®æ–‡ä»¶
        if os.path.isdir(model_path) and (
            os.path.exists(config_path) or os.path.exists(adapter_config_path)
        ):
            # ä½¿ç”¨ ğŸ”§ æ ‡è¯†å¾®è°ƒæ¨¡å‹
            display_name = f"ğŸ”§ {name} (LoRA)"
            finetuned_models[display_name] = model_path

    return finetuned_models


def get_available_models() -> dict:
    """è·å–æ‰€æœ‰å¯ç”¨æ¨¡å‹ï¼ˆåŸºç¡€æ¨¡å‹ + å¾®è°ƒæ¨¡å‹ï¼‰"""
    models = BASE_MODELS.copy()
    models.update(scan_finetuned_models())
    return models


# å¯ç”¨æ¨¡å‹åˆ—è¡¨ï¼ˆåŠ¨æ€æ‰«æï¼‰
AVAILABLE_MODELS = get_available_models()

# ç±»åˆ«é¢œè‰²
CATEGORY_COLORS = {
    "traffic_sign": (255, 0, 0),
    "traffic_light": (0, 255, 0),
    "road_facility": (0, 0, 255),
    "guidance_screen": (255, 165, 0),
    "height_limit": (128, 0, 128),
    "cabinet": (255, 192, 203),
}

CATEGORY_NAMES = {
    "traffic_sign": "äº¤é€šæ ‡å¿—",
    "traffic_light": "ä¿¡å·ç¯",
    "road_facility": "é“è·¯è®¾æ–½",
    "guidance_screen": "è¯±å¯¼å±",
    "height_limit": "é™é«˜æ¶",
    "cabinet": "æœºæŸœ",
}


def is_finetuned_model(model_choice: str) -> bool:
    """åˆ¤æ–­æ˜¯å¦ä¸ºå¾®è°ƒæ¨¡å‹"""
    return model_choice.startswith("ğŸ”§")


def get_base_model_path(finetuned_path: str) -> str:
    """ä»å¾®è°ƒæ¨¡å‹é…ç½®ä¸­è·å–åŸºç¡€æ¨¡å‹è·¯å¾„"""
    config_path = os.path.join(finetuned_path, "finetune_config.json")

    if os.path.exists(config_path):
        try:
            with open(config_path) as f:
                config = json.load(f)
                return config.get("model_path", "./model_cache/Qwen/Qwen3-VL-2B-Instruct")
        except (json.JSONDecodeError, KeyError):
            pass

    # å°è¯•ä» adapter_config.json è¯»å–
    adapter_config_path = os.path.join(finetuned_path, "adapter_config.json")
    if os.path.exists(adapter_config_path):
        try:
            with open(adapter_config_path) as f:
                config = json.load(f)
                return config.get("base_model_name_or_path", "./model_cache/Qwen/Qwen3-VL-2B-Instruct")
        except (json.JSONDecodeError, KeyError):
            pass

    return "./model_cache/Qwen/Qwen3-VL-2B-Instruct"


def get_model_class(model_path: str):
    """æ ¹æ®æ¨¡å‹è·¯å¾„è¿”å›å¯¹åº”çš„æ¨¡å‹ç±»"""
    from transformers import Qwen3VLForConditionalGeneration, Qwen2_5_VLForConditionalGeneration

    model_path_lower = model_path.lower()
    if "qwen3" in model_path_lower:
        return Qwen3VLForConditionalGeneration
    else:
        return Qwen2_5_VLForConditionalGeneration


def load_model(model_choice: str) -> str:
    """åŠ è½½é€‰å®šçš„æ¨¡å‹ï¼ˆæ”¯æŒåŸºç¡€æ¨¡å‹å’Œå¾®è°ƒæ¨¡å‹ï¼‰"""
    global _model, _processor, _current_model_name

    import torch
    from transformers import AutoProcessor

    # åˆ·æ–°æ¨¡å‹åˆ—è¡¨
    global AVAILABLE_MODELS
    AVAILABLE_MODELS = get_available_models()

    if model_choice not in AVAILABLE_MODELS:
        return f"âŒ æœªçŸ¥æ¨¡å‹: {model_choice}"

    model_path = AVAILABLE_MODELS[model_choice]

    # å¦‚æœå·²ç»åŠ è½½äº†ç›¸åŒçš„æ¨¡å‹ï¼Œç›´æ¥è¿”å›
    if _current_model_name == model_choice and _model is not None:
        return f"âœ… æ¨¡å‹ {model_choice} å·²åŠ è½½ï¼Œæ— éœ€é‡å¤åŠ è½½"

    # é‡Šæ”¾æ—§æ¨¡å‹
    if _model is not None:
        del _model
        del _processor
        _model = None
        _processor = None
        torch.cuda.empty_cache()

    try:
        if is_finetuned_model(model_choice):
            # åŠ è½½å¾®è°ƒæ¨¡å‹
            from peft import PeftModel

            base_model_path = get_base_model_path(model_path)
            print(f"Loading base model from: {base_model_path}")
            print(f"Loading LoRA weights from: {model_path}")

            # åŠ è½½åŸºç¡€æ¨¡å‹
            model_class = get_model_class(base_model_path)
            base_model = model_class.from_pretrained(
                base_model_path,
                torch_dtype=torch.bfloat16,
                device_map="auto",
                trust_remote_code=True,
            )

            # åŠ è½½ LoRA æƒé‡å¹¶åˆå¹¶
            _model = PeftModel.from_pretrained(base_model, model_path)
            _model = _model.merge_and_unload()

            # ä¼˜å…ˆä»å¾®è°ƒæ¨¡å‹åŠ è½½ processorï¼Œå¤±è´¥åˆ™ä»åŸºç¡€æ¨¡å‹åŠ è½½
            try:
                _processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)
            except Exception:
                _processor = AutoProcessor.from_pretrained(base_model_path, trust_remote_code=True)

            _current_model_name = model_choice
            return f"âœ… å¾®è°ƒæ¨¡å‹ {model_choice} åŠ è½½æˆåŠŸï¼\n   åŸºç¡€æ¨¡å‹: {base_model_path}"

        else:
            # åŠ è½½åŸºç¡€æ¨¡å‹
            model_class = get_model_class(model_path)
            _processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)

            _model = model_class.from_pretrained(
                model_path,
                torch_dtype=torch.bfloat16,
                device_map="auto",
                trust_remote_code=True,
            )

            _current_model_name = model_choice
            return f"âœ… æ¨¡å‹ {model_choice} åŠ è½½æˆåŠŸï¼"

    except Exception as e:
        _model = None
        _processor = None
        _current_model_name = None
        return f"âŒ åŠ è½½å¤±è´¥: {str(e)}"


def get_model_status() -> str:
    """è·å–å½“å‰æ¨¡å‹çŠ¶æ€"""
    if _current_model_name:
        if is_finetuned_model(_current_model_name):
            return f"å½“å‰æ¨¡å‹: {_current_model_name} (å¾®è°ƒ)"
        return f"å½“å‰æ¨¡å‹: {_current_model_name}"
    return "æœªåŠ è½½æ¨¡å‹"


def refresh_model_list():
    """åˆ·æ–°æ¨¡å‹åˆ—è¡¨"""
    global AVAILABLE_MODELS
    AVAILABLE_MODELS = get_available_models()
    choices = list(AVAILABLE_MODELS.keys())
    return gr.update(choices=choices)


def parse_box_format(text: str) -> dict:
    """
    è§£æ <box>(x1,y1),(x2,y2)</box> æ ¼å¼çš„æ£€æµ‹ç»“æœ
    è¿”å›ä¸ JSON æ ¼å¼å…¼å®¹çš„ç»“æ„
    """
    import re
    detections = []

    # æŒ‰åºå·åˆ†å‰²æ¯ä¸ªæ£€æµ‹é¡¹
    items = re.split(r'(?=\d+\.\s+)', text)

    for item in items:
        if not item.strip():
            continue

        # æå–ç±»åˆ« (åºå·åé¢çš„ç¬¬ä¸€è¡Œ)
        cat_match = re.match(r'(\d+)\.\s*([^\n]+)', item)
        if not cat_match:
            continue

        category = cat_match.group(2).strip()

        # æå–çŠ¶æ€
        status_match = re.search(r'çŠ¶æ€[ï¼š:]\s*([^\n]+)', item)
        status = status_match.group(1).strip() if status_match else "æ­£å¸¸"

        # æå–åŸå› 
        reason_match = re.search(r'åŸå› [ï¼š:]\s*([^\n]+)', item)
        reason = reason_match.group(1).strip() if reason_match else ""

        # æå–åæ ‡
        box_match = re.search(r'<box>\s*\((\d+)\s*,\s*(\d+)\)\s*,\s*\((\d+)\s*,\s*(\d+)\)\s*</box>', item)
        if not box_match:
            continue

        x1, y1, x2, y2 = int(box_match.group(1)), int(box_match.group(2)), int(box_match.group(3)), int(box_match.group(4))

        # åˆ¤æ–­æ˜¯å¦å¼‚å¸¸
        is_anomaly = any(kw in status for kw in ["å¼‚å¸¸", "å…¨ç­", "æŸå", "æ•…éšœ", "ç ´æŸ", "ä¸äº®", "é”™è¯¯", "é»‘å±", "å…¨äº®"])

        detections.append({
            "category": category,
            "anomaly_type": status,
            "confidence": 0.9 if is_anomaly else 0.8,
            "bbox": [x1, y1, x2, y2],
            "description": reason if reason else status,
        })

    has_anomaly = any("å¼‚å¸¸" in d.get("anomaly_type", "") for d in detections)

    return {
        "has_anomaly": has_anomaly,
        "detections": detections,
        "summary": f"æ£€æµ‹åˆ° {len(detections)} ä¸ªç›®æ ‡" + ("ï¼Œå­˜åœ¨å¼‚å¸¸" if has_anomaly else "ï¼Œå‡æ­£å¸¸")
    }


def draw_detections_on_image(image: Image.Image, detections: list, has_anomaly: bool) -> Image.Image:
    """åœ¨å›¾ç‰‡ä¸Šç»˜åˆ¶æ£€æµ‹æ¡†ï¼Œå¼‚å¸¸ç›®æ ‡æ¡†å†…çº¢è‰²é«˜äº®"""
    img = image.copy().convert("RGBA")
    width, height = img.size

    draw = ImageDraw.Draw(img)

    # å°è¯•åŠ è½½å­—ä½“
    try:
        font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf", 20)
    except:
        try:
            font = ImageFont.truetype("/usr/share/fonts/truetype/freefont/FreeSans.ttf", 20)
        except:
            font = ImageFont.load_default()

    for det in detections:
        bbox = det.get("bbox")
        if not bbox or len(bbox) != 4:
            continue

        # åˆ¤æ–­åæ ‡ç±»å‹
        max_coord = max(bbox)
        if max_coord > 1000:
            x1, y1, x2, y2 = int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3])
        else:
            x1 = int(bbox[0] * width / 1000)
            y1 = int(bbox[1] * height / 1000)
            x2 = int(bbox[2] * width / 1000)
            y2 = int(bbox[3] * height / 1000)

        # ç¡®ä¿åæ ‡æœ‰æ•ˆ
        x1, y1 = max(0, x1), max(0, y1)
        x2, y2 = min(width-1, x2), min(height-1, y2)

        # ç¡®ä¿æ¡†æœ‰æ•ˆå°ºå¯¸
        if x2 <= x1 or y2 <= y1:
            continue

        category = det.get("category", "unknown")
        anomaly_type = det.get("anomaly_type", "")
        is_abnormal = anomaly_type.lower() not in ["normal", "æ­£å¸¸", ""]

        # å¼‚å¸¸ç›®æ ‡ï¼šçº¢è‰²é«˜äº®å¡«å…… + çº¢è‰²è¾¹æ¡†
        if is_abnormal:
            # åœ¨ç›®æ ‡æ¡†å†…ç»˜åˆ¶çº¢è‰²åŠé€æ˜å¡«å……ï¼ˆ60% é€æ˜åº¦ï¼‰
            fill_overlay = Image.new("RGBA", (x2-x1, y2-y1), (255, 0, 0, int(255 * 0.6)))
            img.paste(fill_overlay, (x1, y1), fill_overlay)
            color = (255, 0, 0)  # çº¢è‰²è¾¹æ¡†
        else:
            # æ­£å¸¸ç›®æ ‡ï¼šä½¿ç”¨ç±»åˆ«é¢œè‰²
            color = CATEGORY_COLORS.get(category, (0, 255, 0))

        # ç»˜åˆ¶è¾¹ç•Œæ¡†
        draw = ImageDraw.Draw(img)
        draw.rectangle([x1, y1, x2, y2], outline=color, width=3)

        # æ ‡ç­¾æ–‡æœ¬
        cat_name = CATEGORY_NAMES.get(category, category)
        confidence = det.get("confidence", 0)
        label = f"{cat_name}: {anomaly_type} ({confidence:.0%})"

        # ç»˜åˆ¶æ ‡ç­¾èƒŒæ™¯
        text_bbox = draw.textbbox((x1, y1), label, font=font)
        text_w = text_bbox[2] - text_bbox[0]
        text_h = text_bbox[3] - text_bbox[1]
        label_y = max(0, y1 - text_h - 6)

        draw.rectangle([x1, label_y, x1 + text_w + 6, label_y + text_h + 6], fill=color)
        draw.text((x1 + 3, label_y + 3), label, fill=(255, 255, 255), font=font)

    return img.convert("RGB")


def detect(image: Image.Image, prompt: str) -> tuple[Image.Image | None, str, str]:
    """æ‰§è¡Œæ£€æµ‹"""
    import torch
    import re

    if image is None:
        return None, "{}", "è¯·ä¸Šä¼ å›¾ç‰‡"

    if _model is None or _processor is None:
        return None, "{}", "âŒ è¯·å…ˆåŠ è½½æ¨¡å‹"

    # æ„å»ºæ¶ˆæ¯
    messages = [
        {
            "role": "user",
            "content": [
                {"type": "image", "image": image},
                {"type": "text", "text": prompt},
            ],
        }
    ]

    # å¤„ç†è¾“å…¥
    text = _processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = _processor(text=[text], images=[image], padding=True, return_tensors="pt")
    inputs = inputs.to(_model.device)

    # æ¨ç†
    with torch.no_grad():
        output_ids = _model.generate(**inputs, max_new_tokens=2048, do_sample=False)

    # è§£ç 
    output_ids_trimmed = output_ids[0][inputs.input_ids.shape[1]:]
    result_text = _processor.decode(output_ids_trimmed, skip_special_tokens=True)

    # å°è¯•è§£æç»“æœ (æ”¯æŒä¸¤ç§æ ¼å¼)
    json_data = None

    # 1. å…ˆå°è¯•è§£æ JSON æ ¼å¼
    patterns = [r"```json\s*([\s\S]*?)\s*```", r"```\s*([\s\S]*?)\s*```", r"\{[\s\S]*\}"]
    for pattern in patterns:
        matches = re.findall(pattern, result_text)
        for match in matches:
            try:
                parsed = json.loads(match)
                # ç¡®ä¿æ˜¯å­—å…¸æ ¼å¼
                if isinstance(parsed, dict):
                    json_data = parsed
                    break
            except:
                continue
        if json_data:
            break

    # 2. å¦‚æœ JSON è§£æå¤±è´¥ï¼Œå°è¯•è§£æ <box> æ ¼å¼ (å¾®è°ƒæ¨¡å‹è¾“å‡º)
    if not json_data and "<box>" in result_text:
        json_data = parse_box_format(result_text)

    # 3. å¦‚æœéƒ½å¤±è´¥ï¼Œè¿”å›åŸå§‹æ–‡æœ¬
    if not json_data or not json_data.get("detections"):
        return None, result_text, "æ— æ³•è§£ææ£€æµ‹ç»“æœï¼ŒåŸå§‹è¾“å‡ºå¦‚ä¸Š"

    # ç»˜åˆ¶æ£€æµ‹æ¡†
    has_anomaly = json_data.get("has_anomaly", False)
    detections = json_data.get("detections", [])

    # æ£€æŸ¥æ˜¯å¦æœ‰çœŸæ­£çš„å¼‚å¸¸ï¼ˆé normalï¼‰
    real_anomaly = any(
        d.get("anomaly_type", "").lower() not in ["normal", "æ­£å¸¸", ""]
        for d in detections
    )

    annotated_image = draw_detections_on_image(image, detections, real_anomaly)

    # æ ¼å¼åŒ– JSON
    json_str = json.dumps(json_data, ensure_ascii=False, indent=2)

    # ç”Ÿæˆæ‘˜è¦
    summary = json_data.get("summary", "")
    if detections:
        summary += f"\n\næ£€æµ‹åˆ° {len(detections)} ä¸ªç›®æ ‡"
        if real_anomaly:
            summary = "âš ï¸ å‘ç°å¼‚å¸¸ï¼\n\n" + summary

    return annotated_image, json_str, summary


# é»˜è®¤æ£€æµ‹ prompt (JSON æ ¼å¼ - é€‚åˆåŸºç¡€æ¨¡å‹)
DEFAULT_PROMPT_JSON = """è¯·æ£€æµ‹å›¾ç‰‡ä¸­çš„äº¤é€šè®¾å¤‡å¼‚å¸¸ï¼ŒåŒ…æ‹¬ï¼šäº¤é€šæ ‡å¿—ã€ä¿¡å·ç¯ã€é“è·¯è®¾æ–½ã€è¯±å¯¼å±ã€é™é«˜æ¶ã€æœºæŸœç­‰ã€‚

ä»¥JSONæ ¼å¼è¾“å‡ºï¼š
```json
{
  "has_anomaly": true/false,
  "detections": [
    {
      "category": "ç±»åˆ«(traffic_sign/traffic_light/road_facilityç­‰)",
      "anomaly_type": "å¼‚å¸¸ç±»å‹(damaged/normalç­‰)",
      "confidence": 0.9,
      "bbox": [x1, y1, x2, y2],
      "description": "æè¿°"
    }
  ],
  "summary": "æ€»ç»“"
}
```

bboxä¸ºåƒç´ åæ ‡ï¼Œè¯·ç¡®ä¿è¾¹ç•Œæ¡†ç´§å¯†åŒ…å›´ç›®æ ‡ã€‚"""

# å¾®è°ƒæ¨¡å‹ prompt (box æ ¼å¼ - é€‚åˆ LoRA å¾®è°ƒæ¨¡å‹)
DEFAULT_PROMPT_BOX = """è¯·æ£€æµ‹å›¾åƒä¸­çš„äº¤é€šè®¾å¤‡ã€‚éœ€è¦æ£€æµ‹çš„è®¾å¤‡ç±»å‹åŒ…æ‹¬ï¼šäº¤é€šä¿¡å·ç¯ã€äº¤é€šè¯±å¯¼å±ã€é™é«˜æ¶ã€æœºæŸœã€èƒŒåŒ…ç®±ã€‚

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºæ¯ä¸ªæ£€æµ‹åˆ°çš„è®¾å¤‡ï¼š
1. è®¾å¤‡ç±»å‹
2. çŠ¶æ€ï¼ˆæ­£å¸¸/å¼‚å¸¸çŠ¶æ€ï¼‰
3. å¦‚æœå¼‚å¸¸ï¼Œè¯´æ˜å¯èƒ½çš„åŸå› 
4. ä½ç½®åæ ‡ï¼š<box>(x1,y1),(x2,y2)</box>ï¼ˆåæ ‡èŒƒå›´0-1000ï¼‰

å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°ä»»ä½•è®¾å¤‡ï¼Œè¯·å›å¤"æœªæ£€æµ‹åˆ°ç›¸å…³è®¾å¤‡"ã€‚"""

# é»˜è®¤ä½¿ç”¨å¾®è°ƒæ¨¡å‹çš„ prompt
DEFAULT_PROMPT = DEFAULT_PROMPT_BOX


# æ„å»ºç•Œé¢
with gr.Blocks(title="äº¤é€šè®¾å¤‡å¼‚å¸¸æ£€æµ‹", theme=gr.themes.Soft()) as demo:
    gr.Markdown("# ğŸš¦ äº¤é€šè®¾å¤‡å¼‚å¸¸æ£€æµ‹ç³»ç»Ÿ\nåŸºäº Qwen-VL è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆæ”¯æŒåŸºç¡€æ¨¡å‹å’Œ LoRA å¾®è°ƒæ¨¡å‹ï¼‰")

    # æ¨¡å‹åŠ è½½åŒº
    with gr.Row():
        with gr.Column(scale=2):
            model_dropdown = gr.Dropdown(
                choices=list(AVAILABLE_MODELS.keys()),
                value=list(AVAILABLE_MODELS.keys())[0] if AVAILABLE_MODELS else None,
                label="é€‰æ‹©æ¨¡å‹ï¼ˆğŸ”§ è¡¨ç¤ºå¾®è°ƒæ¨¡å‹ï¼‰",
            )
        with gr.Column(scale=1):
            with gr.Row():
                refresh_btn = gr.Button("ğŸ”„ åˆ·æ–°åˆ—è¡¨", variant="secondary", size="sm")
                load_btn = gr.Button("ğŸ“¥ åŠ è½½æ¨¡å‹", variant="primary")
        with gr.Column(scale=2):
            model_status = gr.Textbox(label="æ¨¡å‹çŠ¶æ€", value="æœªåŠ è½½æ¨¡å‹", interactive=False)

    gr.Markdown("---")

    # æ£€æµ‹åŒº
    with gr.Row():
        with gr.Column(scale=1):
            input_image = gr.Image(label="ä¸Šä¼ å›¾ç‰‡", type="pil", height=400)
            prompt_input = gr.Textbox(
                label="æ£€æµ‹æç¤ºè¯",
                value=DEFAULT_PROMPT,
                lines=8,
            )
            detect_btn = gr.Button("ğŸ” å¼€å§‹æ£€æµ‹", variant="primary", size="lg")

        with gr.Column(scale=1):
            output_image = gr.Image(label="æ£€æµ‹ç»“æœ", type="pil", height=400)
            summary_output = gr.Textbox(label="æ£€æµ‹æ‘˜è¦", lines=4)
            json_output = gr.Code(label="JSON ç»“æœ", language="json", lines=10)

    # äº‹ä»¶ç»‘å®š
    refresh_btn.click(
        fn=refresh_model_list,
        inputs=[],
        outputs=[model_dropdown],
    )

    load_btn.click(
        fn=load_model,
        inputs=[model_dropdown],
        outputs=[model_status],
    )

    detect_btn.click(
        fn=detect,
        inputs=[input_image, prompt_input],
        outputs=[output_image, json_output, summary_output],
    )


if __name__ == "__main__":
    print("å¯åŠ¨äº¤é€šè®¾å¤‡å¼‚å¸¸æ£€æµ‹ç³»ç»Ÿ...")
    demo.launch(server_name="0.0.0.0", server_port=7860)
