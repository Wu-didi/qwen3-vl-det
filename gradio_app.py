"""Gradio web interface for traffic equipment anomaly detection."""

import json
import gradio as gr
from PIL import Image, ImageDraw, ImageFont

# å…¨å±€æ¨¡å‹å®ä¾‹
_model = None
_processor = None
_current_model_name = None

# å¯ç”¨æ¨¡å‹åˆ—è¡¨
AVAILABLE_MODELS = {
    "Qwen3-VL-2B": "./model_cache/Qwen/Qwen3-VL-2B-Instruct",
    "Qwen3-VL-8B": "./model_cache/Qwen/Qwen3-VL-8B-Instruct",
    "Qwen2.5-VL-7B": "./model_cache/Qwen/Qwen2.5-VL-7B-Instruct",
}

# ç±»åˆ«é¢œè‰²
CATEGORY_COLORS = {
    "traffic_sign": (255, 0, 0),
    "traffic_light": (0, 255, 0),
    "road_facility": (0, 0, 255),
    "guidance_screen": (255, 165, 0),
    "height_limit": (128, 0, 128),
    "cabinet": (255, 192, 203),
}

CATEGORY_NAMES = {
    "traffic_sign": "äº¤é€šæ ‡å¿—",
    "traffic_light": "ä¿¡å·ç¯",
    "road_facility": "é“è·¯è®¾æ–½",
    "guidance_screen": "è¯±å¯¼å±",
    "height_limit": "é™é«˜æ¶",
    "cabinet": "æœºæŸœ",
}


def load_model(model_choice: str) -> str:
    """åŠ è½½é€‰å®šçš„æ¨¡å‹"""
    global _model, _processor, _current_model_name

    import torch
    from transformers import AutoProcessor, Qwen3VLForConditionalGeneration, Qwen2_5_VLForConditionalGeneration

    if model_choice not in AVAILABLE_MODELS:
        return f"âŒ æœªçŸ¥æ¨¡å‹: {model_choice}"

    model_path = AVAILABLE_MODELS[model_choice]

    # å¦‚æœå·²ç»åŠ è½½äº†ç›¸åŒçš„æ¨¡å‹ï¼Œç›´æ¥è¿”å›
    if _current_model_name == model_choice and _model is not None:
        return f"âœ… æ¨¡å‹ {model_choice} å·²åŠ è½½ï¼Œæ— éœ€é‡å¤åŠ è½½"

    # é‡Šæ”¾æ—§æ¨¡å‹
    if _model is not None:
        del _model
        del _processor
        _model = None
        _processor = None
        torch.cuda.empty_cache()

    try:
        # åŠ è½½ processor
        _processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)

        # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©ç±»
        if "qwen3-vl" in model_path.lower():
            model_class = Qwen3VLForConditionalGeneration
        else:
            model_class = Qwen2_5_VLForConditionalGeneration

        # åŠ è½½æ¨¡å‹
        _model = model_class.from_pretrained(
            model_path,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True,
        )

        _current_model_name = model_choice
        return f"âœ… æ¨¡å‹ {model_choice} åŠ è½½æˆåŠŸï¼"

    except Exception as e:
        _model = None
        _processor = None
        _current_model_name = None
        return f"âŒ åŠ è½½å¤±è´¥: {str(e)}"


def get_model_status() -> str:
    """è·å–å½“å‰æ¨¡å‹çŠ¶æ€"""
    if _current_model_name:
        return f"å½“å‰æ¨¡å‹: {_current_model_name}"
    return "æœªåŠ è½½æ¨¡å‹"


def draw_detections_on_image(image: Image.Image, detections: list, has_anomaly: bool) -> Image.Image:
    """åœ¨å›¾ç‰‡ä¸Šç»˜åˆ¶æ£€æµ‹æ¡†ï¼Œå¼‚å¸¸ç›®æ ‡æ¡†å†…çº¢è‰²é«˜äº®"""
    img = image.copy().convert("RGBA")
    width, height = img.size

    draw = ImageDraw.Draw(img)

    # å°è¯•åŠ è½½å­—ä½“
    try:
        font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf", 20)
    except:
        try:
            font = ImageFont.truetype("/usr/share/fonts/truetype/freefont/FreeSans.ttf", 20)
        except:
            font = ImageFont.load_default()

    for det in detections:
        bbox = det.get("bbox")
        if not bbox or len(bbox) != 4:
            continue

        # åˆ¤æ–­åæ ‡ç±»å‹
        max_coord = max(bbox)
        if max_coord > 1000:
            x1, y1, x2, y2 = int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3])
        else:
            x1 = int(bbox[0] * width / 1000)
            y1 = int(bbox[1] * height / 1000)
            x2 = int(bbox[2] * width / 1000)
            y2 = int(bbox[3] * height / 1000)

        # ç¡®ä¿åæ ‡æœ‰æ•ˆ
        x1, y1 = max(0, x1), max(0, y1)
        x2, y2 = min(width-1, x2), min(height-1, y2)

        # ç¡®ä¿æ¡†æœ‰æ•ˆå°ºå¯¸
        if x2 <= x1 or y2 <= y1:
            continue

        category = det.get("category", "unknown")
        anomaly_type = det.get("anomaly_type", "")
        is_abnormal = anomaly_type.lower() not in ["normal", "æ­£å¸¸", ""]

        # å¼‚å¸¸ç›®æ ‡ï¼šçº¢è‰²é«˜äº®å¡«å…… + çº¢è‰²è¾¹æ¡†
        if is_abnormal:
            # åœ¨ç›®æ ‡æ¡†å†…ç»˜åˆ¶çº¢è‰²åŠé€æ˜å¡«å……ï¼ˆ60% é€æ˜åº¦ï¼‰
            fill_overlay = Image.new("RGBA", (x2-x1, y2-y1), (255, 0, 0, int(255 * 0.6)))
            img.paste(fill_overlay, (x1, y1), fill_overlay)
            color = (255, 0, 0)  # çº¢è‰²è¾¹æ¡†
        else:
            # æ­£å¸¸ç›®æ ‡ï¼šä½¿ç”¨ç±»åˆ«é¢œè‰²
            color = CATEGORY_COLORS.get(category, (0, 255, 0))

        # ç»˜åˆ¶è¾¹ç•Œæ¡†
        draw = ImageDraw.Draw(img)
        draw.rectangle([x1, y1, x2, y2], outline=color, width=3)

        # æ ‡ç­¾æ–‡æœ¬
        cat_name = CATEGORY_NAMES.get(category, category)
        confidence = det.get("confidence", 0)
        label = f"{cat_name}: {anomaly_type} ({confidence:.0%})"

        # ç»˜åˆ¶æ ‡ç­¾èƒŒæ™¯
        text_bbox = draw.textbbox((x1, y1), label, font=font)
        text_w = text_bbox[2] - text_bbox[0]
        text_h = text_bbox[3] - text_bbox[1]
        label_y = max(0, y1 - text_h - 6)

        draw.rectangle([x1, label_y, x1 + text_w + 6, label_y + text_h + 6], fill=color)
        draw.text((x1 + 3, label_y + 3), label, fill=(255, 255, 255), font=font)

    return img.convert("RGB")


def detect(image: Image.Image, prompt: str) -> tuple[Image.Image | None, str, str]:
    """æ‰§è¡Œæ£€æµ‹"""
    import torch
    import re

    if image is None:
        return None, "{}", "è¯·ä¸Šä¼ å›¾ç‰‡"

    if _model is None or _processor is None:
        return None, "{}", "âŒ è¯·å…ˆåŠ è½½æ¨¡å‹"

    # æ„å»ºæ¶ˆæ¯
    messages = [
        {
            "role": "user",
            "content": [
                {"type": "image", "image": image},
                {"type": "text", "text": prompt},
            ],
        }
    ]

    # å¤„ç†è¾“å…¥
    text = _processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = _processor(text=[text], images=[image], padding=True, return_tensors="pt")
    inputs = inputs.to(_model.device)

    # æ¨ç†
    with torch.no_grad():
        output_ids = _model.generate(**inputs, max_new_tokens=2048, do_sample=False)

    # è§£ç 
    output_ids_trimmed = output_ids[0][inputs.input_ids.shape[1]:]
    result_text = _processor.decode(output_ids_trimmed, skip_special_tokens=True)

    # è§£æ JSON
    json_data = None
    patterns = [r"```json\s*([\s\S]*?)\s*```", r"```\s*([\s\S]*?)\s*```", r"\{[\s\S]*\}"]
    for pattern in patterns:
        matches = re.findall(pattern, result_text)
        for match in matches:
            try:
                json_data = json.loads(match)
                break
            except:
                continue
        if json_data:
            break

    if not json_data:
        return None, result_text, "æ— æ³•è§£ææ£€æµ‹ç»“æœ"

    # ç»˜åˆ¶æ£€æµ‹æ¡†
    has_anomaly = json_data.get("has_anomaly", False)
    detections = json_data.get("detections", [])

    # æ£€æŸ¥æ˜¯å¦æœ‰çœŸæ­£çš„å¼‚å¸¸ï¼ˆé normalï¼‰
    real_anomaly = any(
        d.get("anomaly_type", "").lower() not in ["normal", "æ­£å¸¸", ""]
        for d in detections
    )

    annotated_image = draw_detections_on_image(image, detections, real_anomaly)

    # æ ¼å¼åŒ– JSON
    json_str = json.dumps(json_data, ensure_ascii=False, indent=2)

    # ç”Ÿæˆæ‘˜è¦
    summary = json_data.get("summary", "")
    if detections:
        summary += f"\n\næ£€æµ‹åˆ° {len(detections)} ä¸ªç›®æ ‡"
        if real_anomaly:
            summary = "âš ï¸ å‘ç°å¼‚å¸¸ï¼\n\n" + summary

    return annotated_image, json_str, summary


# é»˜è®¤æ£€æµ‹ prompt
DEFAULT_PROMPT = """è¯·æ£€æµ‹å›¾ç‰‡ä¸­çš„äº¤é€šè®¾å¤‡å¼‚å¸¸ï¼ŒåŒ…æ‹¬ï¼šäº¤é€šæ ‡å¿—ã€ä¿¡å·ç¯ã€é“è·¯è®¾æ–½ã€è¯±å¯¼å±ã€é™é«˜æ¶ã€æœºæŸœç­‰ã€‚

ä»¥JSONæ ¼å¼è¾“å‡ºï¼š
```json
{
  "has_anomaly": true/false,
  "detections": [
    {
      "category": "ç±»åˆ«(traffic_sign/traffic_light/road_facilityç­‰)",
      "anomaly_type": "å¼‚å¸¸ç±»å‹(damaged/normalç­‰)",
      "confidence": 0.9,
      "bbox": [x1, y1, x2, y2],
      "description": "æè¿°"
    }
  ],
  "summary": "æ€»ç»“"
}
```

bboxä¸ºåƒç´ åæ ‡ï¼Œè¯·ç¡®ä¿è¾¹ç•Œæ¡†ç´§å¯†åŒ…å›´ç›®æ ‡ã€‚"""


# æ„å»ºç•Œé¢
with gr.Blocks(title="äº¤é€šè®¾å¤‡å¼‚å¸¸æ£€æµ‹", theme=gr.themes.Soft()) as demo:
    gr.Markdown("# ğŸš¦ äº¤é€šè®¾å¤‡å¼‚å¸¸æ£€æµ‹ç³»ç»Ÿ\nåŸºäº Qwen3-VL è§†è§‰è¯­è¨€æ¨¡å‹")

    # æ¨¡å‹åŠ è½½åŒº
    with gr.Row():
        with gr.Column(scale=2):
            model_dropdown = gr.Dropdown(
                choices=list(AVAILABLE_MODELS.keys()),
                value="Qwen3-VL-8B",
                label="é€‰æ‹©æ¨¡å‹",
            )
        with gr.Column(scale=1):
            load_btn = gr.Button("ğŸ”„ åŠ è½½æ¨¡å‹", variant="primary")
        with gr.Column(scale=2):
            model_status = gr.Textbox(label="æ¨¡å‹çŠ¶æ€", value="æœªåŠ è½½æ¨¡å‹", interactive=False)

    gr.Markdown("---")

    # æ£€æµ‹åŒº
    with gr.Row():
        with gr.Column(scale=1):
            input_image = gr.Image(label="ä¸Šä¼ å›¾ç‰‡", type="pil", height=400)
            prompt_input = gr.Textbox(
                label="æ£€æµ‹æç¤ºè¯",
                value=DEFAULT_PROMPT,
                lines=8,
            )
            detect_btn = gr.Button("ğŸ” å¼€å§‹æ£€æµ‹", variant="primary", size="lg")

        with gr.Column(scale=1):
            output_image = gr.Image(label="æ£€æµ‹ç»“æœ", type="pil", height=400)
            summary_output = gr.Textbox(label="æ£€æµ‹æ‘˜è¦", lines=4)
            json_output = gr.Code(label="JSON ç»“æœ", language="json", lines=10)

    # äº‹ä»¶ç»‘å®š
    load_btn.click(
        fn=load_model,
        inputs=[model_dropdown],
        outputs=[model_status],
    )

    detect_btn.click(
        fn=detect,
        inputs=[input_image, prompt_input],
        outputs=[output_image, json_output, summary_output],
    )


if __name__ == "__main__":
    print("å¯åŠ¨äº¤é€šè®¾å¤‡å¼‚å¸¸æ£€æµ‹ç³»ç»Ÿ...")
    demo.launch(server_name="0.0.0.0", server_port=7860)
