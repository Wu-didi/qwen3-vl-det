#!/bin/bash
# GRPO è®­ç»ƒé€Ÿåº¦ä¼˜åŒ–é…ç½®
# é’ˆå¯¹ä¸åŒåœºæ™¯çš„æ¨èé…ç½®

echo "=========================================="
echo "GRPO è®­ç»ƒé€Ÿåº¦ä¼˜åŒ–å»ºè®®"
echo "=========================================="

cat << 'EOF'

## ğŸŒ ä¸ºä»€ä¹ˆ GRPO è®­ç»ƒæ…¢ï¼Ÿ

1. **åœ¨çº¿ç”Ÿæˆå“åº”**ï¼šæ¯ä¸ªæ ·æœ¬éœ€è¦ç”Ÿæˆå¤šä¸ªå“åº”ï¼ˆé»˜è®¤ 4 ä¸ªï¼‰
2. **å¤šæ¬¡å‰å‘ä¼ æ’­**ï¼š
   - ç”Ÿæˆ 4 ä¸ªå“åº”ï¼š4 æ¬¡ç”Ÿæˆ
   - è®¡ç®— policy log probsï¼š4 æ¬¡å‰å‘
   - è®¡ç®— reference log probsï¼š4 æ¬¡å‰å‘
   - æ€»å…±çº¦ 12 æ¬¡å‰å‘ä¼ æ’­ vs ç›‘ç£å­¦ä¹ çš„ 1 æ¬¡

3. **å›¾ç‰‡ token æ•°é‡**ï¼š
   - Qwen-VL å›¾ç‰‡ token ä¸å›¾ç‰‡å¤§å°æˆå¹³æ–¹å…³ç³»
   - 1024px å›¾ç‰‡ â‰ˆ 4000+ tokens
   - 512px å›¾ç‰‡ â‰ˆ 1000 tokens
   - 384px å›¾ç‰‡ â‰ˆ 600 tokens

## âš¡ ä¼˜åŒ–é…ç½®ï¼ˆæŒ‰åœºæ™¯ï¼‰

### åœºæ™¯ 1: å¿«é€Ÿå®éªŒ/è°ƒè¯•ï¼ˆæœ€å¿«ï¼‰
MAX_IMAGE_SIZE=384
NUM_GENERATIONS=2
LORA_R=32
EVAL_STEPS=0
MODEL_PATH="Qwen3-VL-2B-Instruct"

é¢„è®¡é€Ÿåº¦ï¼š~30-60 ç§’/æ ·æœ¬ï¼ˆ2B æ¨¡å‹ï¼‰
é€‚ç”¨ï¼šå¿«é€ŸéªŒè¯æƒ³æ³•ã€è°ƒè¯•ä»£ç 

### åœºæ™¯ 2: å¹³è¡¡é€Ÿåº¦å’Œæ•ˆæœï¼ˆæ¨èï¼‰
MAX_IMAGE_SIZE=512
NUM_GENERATIONS=4
LORA_R=64
EVAL_STEPS=0
MODEL_PATH="Qwen3-VL-2B-Instruct"

é¢„è®¡é€Ÿåº¦ï¼š~60-120 ç§’/æ ·æœ¬ï¼ˆ2B æ¨¡å‹ï¼‰
é€‚ç”¨ï¼šæ­£å¸¸è®­ç»ƒ

### åœºæ™¯ 3: è¿½æ±‚æœ€ä½³æ•ˆæœï¼ˆæ…¢ï¼‰
MAX_IMAGE_SIZE=768
NUM_GENERATIONS=6
LORA_R=64
EVAL_STEPS=200
MODEL_PATH="Qwen3-VL-8B-Instruct"

é¢„è®¡é€Ÿåº¦ï¼š~300-600 ç§’/æ ·æœ¬ï¼ˆ8B æ¨¡å‹ï¼‰
é€‚ç”¨ï¼šæœ€ç»ˆæ¨¡å‹è®­ç»ƒ

### åœºæ™¯ 4: ä½ å½“å‰çš„é…ç½®ï¼ˆéå¸¸æ…¢ï¼‰
MAX_IMAGE_SIZE=1024  # âš ï¸ å¤ªå¤§ï¼
NUM_GENERATIONS=4
LORA_R=64
EVAL_STEPS=200
MODEL_PATH="Qwen3-VL-8B-Instruct"

é¢„è®¡é€Ÿåº¦ï¼š~600-1200 ç§’/æ ·æœ¬ï¼ˆ8B æ¨¡å‹ï¼‰
é—®é¢˜ï¼šå›¾ç‰‡å¤ªå¤§ + 8B æ¨¡å‹ + é¢‘ç¹éªŒè¯

## ğŸ“Š é€Ÿåº¦å¯¹æ¯”ï¼ˆç›¸å¯¹äºåŸºå‡†ï¼‰

é…ç½®é¡¹                    | é€Ÿåº¦å½±å“ | å»ºè®®
-------------------------|---------|-----
MAX_IMAGE_SIZE=384       | 1x      | å¿«é€Ÿå®éªŒ
MAX_IMAGE_SIZE=512       | 2x      | æ¨è
MAX_IMAGE_SIZE=768       | 4x      | é«˜è´¨é‡
MAX_IMAGE_SIZE=1024      | 8x      | âš ï¸ å¤ªæ…¢
NUM_GENERATIONS=2        | 1x      | æœ€å¿«
NUM_GENERATIONS=4        | 2x      | æ¨è
NUM_GENERATIONS=6        | 3x      | é«˜è´¨é‡
2B æ¨¡å‹                  | 1x      | æ¨è
8B æ¨¡å‹                  | 4x      | é«˜è´¨é‡
LORA_R=32               | 1x      | å¿«é€Ÿ
LORA_R=64               | 1.2x    | æ¨è
EVAL_STEPS=0            | 1x      | è®­ç»ƒæ—¶ç¦ç”¨
EVAL_STEPS=200          | 1.5x    | éœ€è¦éªŒè¯æ—¶

## ğŸ¯ ç«‹å³ä¼˜åŒ–å»ºè®®

### 1. é™ä½å›¾ç‰‡å¤§å°ï¼ˆæœ€é‡è¦ï¼ï¼‰
sed -i 's/MAX_IMAGE_SIZE=1024/MAX_IMAGE_SIZE=512/' scripts/run/train_grpo_trl.sh

### 2. ç¦ç”¨è®­ç»ƒæ—¶éªŒè¯
sed -i 's/EVAL_STEPS=200/EVAL_STEPS=0/' scripts/run/train_grpo_trl.sh

### 3. å‡å°‘ç”Ÿæˆæ•°é‡ï¼ˆå¯é€‰ï¼‰
sed -i 's/NUM_GENERATIONS=4/NUM_GENERATIONS=2/' scripts/run/train_grpo_trl.sh

### 4. ä½¿ç”¨ 2B æ¨¡å‹æµ‹è¯•ï¼ˆå¯é€‰ï¼‰
sed -i 's/Qwen3-VL-8B-Instruct/Qwen3-VL-2B-Instruct/' scripts/run/train_grpo_trl.sh

## ğŸ’¡ å…¶ä»–ä¼˜åŒ–æŠ€å·§

### 1. ä½¿ç”¨æ›´å°çš„ LoRA rank
LORA_R=32  # ä» 64 é™åˆ° 32ï¼Œé€Ÿåº¦æå‡ ~20%

### 2. å‡å°‘ max_completion_length
MAX_COMPLETION_LENGTH=256  # ä» 512 é™åˆ° 256

### 3. å¢å¤§ gradient_accumulation
GRADIENT_ACCUMULATION=8  # ä» 4 å¢åˆ° 8ï¼Œå‡å°‘ä¼˜åŒ–æ­¥éª¤

### 4. ä½¿ç”¨æ›´å°‘çš„è®­ç»ƒæ•°æ®
# å…ˆç”¨ 10% æ•°æ®å¿«é€ŸéªŒè¯æ•ˆæœ
head -n 100 train.json > train_small.json

## ğŸ“ˆ é¢„æœŸè®­ç»ƒæ—¶é—´ä¼°ç®—

å‡è®¾æœ‰ 1000 ä¸ªè®­ç»ƒæ ·æœ¬ï¼š

é…ç½®                          | æ¯æ ·æœ¬æ—¶é—´ | æ€»æ—¶é—´ï¼ˆ1 epochï¼‰
-----------------------------|-----------|------------------
å½“å‰é…ç½®ï¼ˆ1024px + 8Bï¼‰       | ~10 åˆ†é’Ÿ  | ~7 å¤©
ä¼˜åŒ–åï¼ˆ512px + 8Bï¼‰          | ~2 åˆ†é’Ÿ   | ~1.5 å¤©
æ¨èé…ç½®ï¼ˆ512px + 2Bï¼‰        | ~1 åˆ†é’Ÿ   | ~17 å°æ—¶
å¿«é€Ÿé…ç½®ï¼ˆ384px + 2Bï¼‰        | ~30 ç§’    | ~8 å°æ—¶

## ğŸ” ç›‘æ§è®­ç»ƒé€Ÿåº¦

### æŸ¥çœ‹å½“å‰é€Ÿåº¦
tail -f outputs/qwen3vl_grpo_trl/training_log.json | grep step

### è®¡ç®—å¹³å‡é€Ÿåº¦
python << 'PYTHON'
import json
import time

log_file = "outputs/qwen3vl_grpo_trl/training_log.json"
try:
    with open(log_file) as f:
        log = json.load(f)

    history = log.get("train_history", [])
    if len(history) >= 2:
        # ä¼°ç®—æ¯æ­¥æ—¶é—´
        steps = [h["step"] for h in history]
        # å‡è®¾æ¯æ­¥å¤„ç† 1 ä¸ªæ ·æœ¬ï¼ˆbatch_size=1, gradient_accumulation=4ï¼‰
        samples_per_step = 4  # gradient_accumulation

        print(f"å·²å®Œæˆ {len(history)} ä¸ªè®°å½•ç‚¹")
        print(f"æœ€æ–° step: {history[-1]['step']}")
        print(f"ä¼°ç®—ï¼šæ¯ {samples_per_step} ä¸ªæ ·æœ¬è®°å½•ä¸€æ¬¡")
except FileNotFoundError:
    print("è®­ç»ƒæ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè®­ç»ƒå¯èƒ½è¿˜æœªå¼€å§‹")
PYTHON

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **å›¾ç‰‡å¤§å° vs æ£€æµ‹ç²¾åº¦**
   - 512px å¯¹å¤§å¤šæ•°åœºæ™¯è¶³å¤Ÿ
   - åªæœ‰éœ€è¦æ£€æµ‹å°ç›®æ ‡æ—¶æ‰ç”¨ 768px+
   - 1024px é€šå¸¸æ²¡å¿…è¦ï¼Œæ€§ä»·æ¯”ä½

2. **NUM_GENERATIONS vs è®­ç»ƒæ•ˆæœ**
   - 2: æœ€å¿«ï¼Œä½†å¯èƒ½ä¸ç¨³å®š
   - 4: æ¨èï¼Œå¹³è¡¡é€Ÿåº¦å’Œæ•ˆæœ
   - 6+: æ›´ç¨³å®šï¼Œä½†æ”¶ç›Šé€’å‡

3. **éªŒè¯é¢‘ç‡**
   - è®­ç»ƒæ—¶å¯ä»¥ç¦ç”¨éªŒè¯ï¼ˆEVAL_STEPS=0ï¼‰
   - è®­ç»ƒå®Œæˆåå•ç‹¬è¿è¡ŒéªŒè¯
   - æˆ–è€…è®¾ç½®æ›´å¤§çš„ EVAL_STEPSï¼ˆå¦‚ 500ï¼‰

4. **æ¨¡å‹é€‰æ‹©**
   - å…ˆç”¨ 2B æ¨¡å‹éªŒè¯æ•ˆæœ
   - ç¡®è®¤æœ‰æ•ˆåå†ç”¨ 8B æ¨¡å‹
   - 8B æ¨¡å‹æ•ˆæœæå‡é€šå¸¸ < 20%ï¼Œä½†æ—¶é—´å¢åŠ  4 å€

## ğŸš€ å¿«é€Ÿåº”ç”¨ä¼˜åŒ–

# å¤‡ä»½å½“å‰é…ç½®
cp scripts/run/train_grpo_trl.sh scripts/run/train_grpo_trl.sh.backup

# åº”ç”¨æ¨èé…ç½®
cat > scripts/run/train_grpo_trl_fast.sh << 'SCRIPT'
#!/bin/bash
# GRPO å¿«é€Ÿè®­ç»ƒé…ç½®

set -e

CUDA_DEVICES=7
MAX_IMAGE_SIZE=512                 # ä» 1024 é™åˆ° 512ï¼ˆé€Ÿåº¦æå‡ 4 å€ï¼‰
BATCH_SIZE=1
NUM_GENERATIONS=4                  # ä¿æŒ 4 ä¸ªç”Ÿæˆ
GRADIENT_ACCUMULATION=4
LORA_R=64
NUM_EPOCHS=1
LEARNING_RATE=5e-6

TEMPERATURE=0.7
BETA=0.5

MODEL_PATH="./model_cache/Qwen/Qwen3-VL-8B-Instruct"
SFT_MODEL_PATH="/mnt/home/wudidi/code_v5/qwen3-vl-det/outputs/qwen3vl_lora_exp3"
TRAIN_DATA="data/hefei_last_dataset/qwen_data/train.json"
VAL_DATA=""  # è®­ç»ƒæ—¶ç¦ç”¨éªŒè¯
OUTPUT_DIR="outputs/qwen3vl_grpo_trl_fast"

LORA_ALPHA=16
LORA_DROPOUT=0.1
MAX_COMPLETION_LENGTH=512
MAX_PROMPT_LENGTH=1024
SAVE_STEPS=200
EVAL_STEPS=0                       # ç¦ç”¨éªŒè¯ä»¥åŠ é€Ÿ
LOGGING_STEPS=10

DISABLE_4BIT=false
DISABLE_BF16=false

export CUDA_VISIBLE_DEVICES=$CUDA_DEVICES
export LD_LIBRARY_PATH=/opt/conda/lib/python3.11/site-packages/nvidia/cuda_runtime/lib:$LD_LIBRARY_PATH
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

cd "$(dirname "$0")/../.."

echo "=========================================="
echo "GRPO å¿«é€Ÿè®­ç»ƒé…ç½®"
echo "=========================================="
echo "ä¼˜åŒ–é¡¹ï¼š"
echo "  - å›¾ç‰‡å¤§å°: 1024 -> 512 (é€Ÿåº¦æå‡ 4x)"
echo "  - ç¦ç”¨éªŒè¯: EVAL_STEPS=0"
echo "  - é¢„è®¡é€Ÿåº¦: ~2 åˆ†é’Ÿ/æ ·æœ¬ (8B æ¨¡å‹)"
echo "=========================================="

CMD="python scripts/training/grpo_finetune_trl.py \
    --model_path $MODEL_PATH \
    --train_data $TRAIN_DATA \
    --output_dir $OUTPUT_DIR \
    --max_image_size $MAX_IMAGE_SIZE \
    --batch_size $BATCH_SIZE \
    --num_generations $NUM_GENERATIONS \
    --lora_r $LORA_R \
    --lora_alpha $LORA_ALPHA \
    --lora_dropout $LORA_DROPOUT \
    --temperature $TEMPERATURE \
    --beta $BETA \
    --num_epochs $NUM_EPOCHS \
    --gradient_accumulation_steps $GRADIENT_ACCUMULATION \
    --learning_rate $LEARNING_RATE \
    --max_completion_length $MAX_COMPLETION_LENGTH \
    --max_prompt_length $MAX_PROMPT_LENGTH \
    --save_steps $SAVE_STEPS \
    --eval_steps $EVAL_STEPS \
    --logging_steps $LOGGING_STEPS"

if [ "$DISABLE_4BIT" = "true" ]; then
    CMD="$CMD --no_4bit"
fi

if [ "$DISABLE_BF16" = "true" ]; then
    CMD="$CMD --no_bf16"
fi

if [ -n "$SFT_MODEL_PATH" ]; then
    CMD="$CMD --sft_model_path $SFT_MODEL_PATH"
fi

eval $CMD
SCRIPT

chmod +x scripts/run/train_grpo_trl_fast.sh

echo ""
echo "âœ… å·²åˆ›å»ºä¼˜åŒ–é…ç½®: scripts/run/train_grpo_trl_fast.sh"
echo ""
echo "ä½¿ç”¨æ–¹æ³•ï¼š"
echo "  ./scripts/run/train_grpo_trl_fast.sh"
echo ""

EOF
